
(라이브러리와 데이터셋은 동일하여 생략)
### LSTM 모델 생성
```python
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, vocab_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(vocab_size, hidden_size, num_layers, batch_first=True)  # LSTM 레이어
        self.fc = nn.Linear(hidden_size, output_size)  # 완전 연결층

    def forward(self, x, hidden):
        out, hidden = self.lstm(x, hidden)  # LSTM 순전파
        out = self.fc(out[:, -1, :])  # 마지막 시퀀스 출력만 사용
        return out, hidden

    def init_hidden(self, batch_size):
        # 초기 hidden state 설정
        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),
                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))
```

```python
print(model)
```

<pre>
LSTMModel(
  (lstm): LSTM(65, 256, num_layers=2, batch_first=True)
  (fc): Linear(in_features=256, out_features=65, bias=True)
)
</pre>

### 모델 훈련 함수
```python
def train_model(model, dataloader, criterion, optimizer, num_epochs=20):
    model.train()  # 모델을 훈련 모드로 설정
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            inputs = nn.functional.one_hot(inputs, num_classes=vocab_size).float().to(device)  # 원-핫 인코딩 및 GPU로 이동
            labels = labels.to(device)

            hidden = model.init_hidden(inputs.size(0))  # 각 배치마다 hidden 상태 초기화

            # 옵티마이저 초기화
            optimizer.zero_grad()

            # 순전파, 역전파, 최적화
            outputs, hidden = model(inputs, hidden)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # hidden 상태를 detach하여 그래프의 연결을 끊음
            hidden = (hidden[0].detach(), hidden[1].detach())

        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}')

    print('Finished Training')
```

### 텍스트 생성 함수
```python
def generate_text(model, start_str, length, temperature=1.0):
    model.eval()  # 모델을 평가 모드로 설정
    hidden = model.init_hidden(1)  # 초기 hidden 상태 설정

    input_seq = torch.tensor([char_to_idx[char] for char in start_str]).unsqueeze(0).to(device)
    generated_text = start_str

    with torch.no_grad():
        for _ in range(length):
            input_seq = nn.functional.one_hot(input_seq, num_classes=vocab_size).float()
            output, hidden = model(input_seq, hidden)

            # 다음 문자를 샘플링
            output = output.squeeze().div(temperature).exp().cpu()
            top_char = torch.multinomial(output, 1)[0]

            generated_char = idx_to_char[top_char.item()]
            generated_text += generated_char

            # 다음 입력 시퀀스 준비
            input_seq = torch.tensor([[top_char]]).to(device)

    return generated_text
```



```python
# 모델 훈련
train_model(model, dataloader, criterion, optimizer, num_epochs=5)
```

<pre>
Epoch 1/5, Loss: 1.7038
Epoch 2/5, Loss: 1.3778
Epoch 3/5, Loss: 1.3028
Epoch 4/5, Loss: 1.2580
Epoch 5/5, Loss: 1.2250
Finished Training
</pre>

```python
# 테스트 시작 문자열 및 생성할 텍스트 길이
start_str = "To be, or not to be, that is the question:"
length = 200

# 텍스트 생성
generated_text = generate_text(model, start_str, length, temperature=0.8)
print(generated_text)

```

<pre>
To be, or not to be, that is the question:
Which I will give myself for you,
You make him here, if nothing lie in exile:
I will, sir.

GREMIO:
Nay, is your aim.

PETRUCHIO:
It may not, sir:
They are sweetest while they are all mine honour:
Th
</pre>


앞선 RNN모델보다 더 그럴듯한 생성을 보여주고 있다. 단어도 대부분 올바른 형태를 띄고 있다.
