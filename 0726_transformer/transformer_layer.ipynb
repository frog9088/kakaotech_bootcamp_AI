{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, attention_size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.attention_size = attention_size\n",
        "\n",
        "        self.V = nn.Linear(self.embed_size, self.attention_size, bias=False) #embed_size * attention_size\n",
        "        self.K = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
        "        self.Q = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.Q(x)\n",
        "        key = self.K(x)\n",
        "        value = self.V(x) #(seq_len, embeded_size) * (embed_size, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        attention_score = F.softmax(query @ key.transpose(-2,-1) / (self.embed_size ** (1 / 2)), dim = -1)\n",
        "        #(seq_len, attention_size) * (attention_size, seq_len) -> (seq_len, seq_len)\n",
        "        out = attention_score @ value #(seq_len, seq_len) * (seq_len, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        return out, key, value"
      ],
      "metadata": {
        "id": "a-I-Xb-h-DJS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_self_attention():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    embed_size = 64\n",
        "    attention_size = 32\n",
        "\n",
        "    # Create an instance of SelfAttention\n",
        "    self_attention = SelfAttention(embed_size, attention_size)\n",
        "\n",
        "    # Create a random input tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output, _, _ = self_attention(x)\n",
        "\n",
        "    # Print shapes\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check if output shape is correct\n",
        "    assert output.shape == (batch_size, seq_length, attention_size), \"Output shape is incorrect\"\n",
        "\n",
        "    # Check if output values are finite\n",
        "    assert torch.isfinite(output).all(), \"Output contains non-finite values\"\n",
        "\n",
        "    print(\"All checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_self_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Pp2IL2KK4G",
        "outputId": "f05887f6-50ff-40f5-d6e6-acac4561dcdf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 10, 64])\n",
            "Output shape: torch.Size([32, 10, 32])\n",
            "All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_num):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.attention_size = self.embed_size // self.head_num\n",
        "\n",
        "        self.multi_head_attention = nn.ModuleList([\n",
        "            SelfAttention(self.embed_size, self.attention_size)\n",
        "            for _ in range(self.head_num)\n",
        "        ]) # (seq_len, attention_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.cat([h(x)[0] for h in self.multi_head_attention], dim=-1) # (seq_len, head_num * attention_size) + add(reisudal)\n",
        "        x += output\n",
        "        key = torch.cat([h(x)[1] for h in self.multi_head_attention], dim=-1)\n",
        "        value = torch.cat([h(x)[2] for h in self.multi_head_attention], dim=-1)\n",
        "        x = F.normalize(x, dim=-1) #normalization\n",
        "\n",
        "        return x, key, value"
      ],
      "metadata": {
        "id": "pCSyDUFSOEdJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_head_attention():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    embed_size = 64\n",
        "    head_num = 8\n",
        "\n",
        "    # Create an instance of SelfAttention\n",
        "    multi_head_attention = MultiHeadAttention(embed_size, head_num)\n",
        "\n",
        "    # Create a random input tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output, _, _ = multi_head_attention(x)\n",
        "\n",
        "    # Print shapes\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check if output shape is correct\n",
        "    assert output.shape == (batch_size, seq_length, embed_size), \"Output shape is incorrect\"\n",
        "\n",
        "    # Check if output values are finite\n",
        "    assert torch.isfinite(output).all(), \"Output contains non-finite values\"\n",
        "\n",
        "    print(\"All checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjqUadahCOKn",
        "outputId": "85baa40f-246d-4092-a244-09d416c99697"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 10, 64])\n",
            "Output shape: torch.Size([32, 10, 64])\n",
            "All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc1 = nn.Linear(self.embed_size, self.hidden_size)\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.hidden_size, self.embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.Relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SaxbLt1aCr7a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_feed_forward():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    embed_size = 64\n",
        "    head_num = 8\n",
        "\n",
        "    # Create an instance of SelfAttention\n",
        "    multi_head_attention = MultiHeadAttention(embed_size, head_num)\n",
        "\n",
        "    # Create a random input tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output = multi_head_attention(x)\n",
        "\n",
        "    # Print shapes\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check if output shape is correct\n",
        "    assert output.shape == (batch_size, seq_length, embed_size), \"Output shape is incorrect\"\n",
        "\n",
        "    # Check if output values are finite\n",
        "    assert torch.isfinite(output).all(), \"Output contains non-finite values\"\n",
        "\n",
        "    print(\"All checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1TupvfQvVSn",
        "outputId": "629bc21a-4dfb-4d59-e7d2-8dab6dfdf365"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 10, 64])\n",
            "Output shape: torch.Size([32, 10, 64])\n",
            "All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedAttention(nn.Module):\n",
        "    def __init__(self, embed_size, attention_size):\n",
        "        super(MaskedAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.attention_size = attention_size\n",
        "\n",
        "        self.V = nn.Linear(self.embed_size, self.attention_size, bias=False) #embed_size * attention_size\n",
        "        self.K = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
        "        self.Q = nn.Linear(self.embed_size, self.attention_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.Q(x)\n",
        "        key = self.K(x)\n",
        "        value = self.V(x) #(seq_len, embeded_size) * (embed_size, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        attention_score = F.softmax(query @ key.transpose(-2,-1) / (self.embed_size ** (1 / 2)), dim = -1)\n",
        "        #(seq_len, attention_size) * (attention_size, seq_len) -> (seq_len, seq_len)\n",
        "        subsequent_mask = torch.triu(torch.ones(attention_score.shape), diagonal=1) #masking\n",
        "        attention_score = attention_score.masked_fill(subsequent_mask == 1, float(\"-1e20\")) #masking\n",
        "        out = attention_score @ value #(seq_len, seq_len) * (seq_len, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        return out, query"
      ],
      "metadata": {
        "id": "5PCNQTgs1U6j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_num):\n",
        "        super(MaskedMultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.head_num = head_num\n",
        "        self.attention_size = self.embed_size // self.head_num\n",
        "\n",
        "        self.multi_head_attention = nn.ModuleList([\n",
        "            MaskedAttention(self.embed_size, self.attention_size)\n",
        "            for _ in range(self.head_num)\n",
        "        ]) # (seq_len, attention_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.cat([h(x)[0] for h in self.multi_head_attention], dim=-1) # (seq_len, head_num * attention_size) + add(reisudal)\n",
        "        x += output\n",
        "        query = torch.cat([h(x)[1] for h in self.multi_head_attention], dim=-1)\n",
        "        x = F.normalize(x, dim=-1) #normalization\n",
        "\n",
        "        return x, query"
      ],
      "metadata": {
        "id": "k51G3945zBoa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_attention_mask(input_ids):\n",
        "\n",
        "        # 다음 데이터를 예측에 활용하지 못하도록 look-ahead mask 생성(extended_attention_mask)\n",
        "        # 생성한 mask를 encoder의 input으로 활용 -> self-attention 과정에서 사용됨\n",
        "        attention_mask = (input_ids > 0).long()\n",
        "        print(attention_mask) # 1\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.int64\n",
        "        print(extended_attention_mask) # 2\n",
        "        max_len = attention_mask.size(-1)\n",
        "        attn_shape = (1, max_len, max_len)\n",
        "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)  # torch.uint8\n",
        "        print(subsequent_mask) # 3\n",
        "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
        "        subsequent_mask = subsequent_mask.long()\n",
        "\n",
        "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
        "        # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        return extended_attention_mask"
      ],
      "metadata": {
        "id": "13cEJGlE2pVk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_masekd_multi_head_attention():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    embed_size = 64\n",
        "    head_num = 8\n",
        "\n",
        "    # Create an instance of SelfAttention\n",
        "    multi_head_attention = MaskedMultiHeadAttention(embed_size, head_num)\n",
        "\n",
        "    # Create a random input tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output, _ = multi_head_attention(x)\n",
        "\n",
        "    # Print shapes\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check if output shape is correct\n",
        "    assert output.shape == (batch_size, seq_length, embed_size), \"Output shape is incorrect\"\n",
        "\n",
        "    # Check if output values are finite\n",
        "    assert torch.isfinite(output).all(), \"Output contains non-finite values\"\n",
        "\n",
        "    print(\"All checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_masekd_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUSG90eJ5Cbx",
        "outputId": "f96e3f3a-e297-4274-9f9d-710040b6cbb5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 10, 64])\n",
            "Output shape: torch.Size([32, 10, 64])\n",
            "All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResidualConnection, self).__init__()\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        out = x + residual\n",
        "        out = F.normalize(out, dim=-1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "p-xEFETsY1t8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoderAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_num, K, V):\n",
        "        super(EncoderDecoderAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.attention_size = embed_size // head_num\n",
        "\n",
        "        self.V = V #embed_size * attention_size\n",
        "        self.K = K\n",
        "\n",
        "\n",
        "    def forward(self, Q):\n",
        "        query = Q\n",
        "        key = self.K\n",
        "        value = self.V #(seq_len, embeded_size) * (embed_size, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        attention_score = F.softmax(query @ key.transpose(-2,-1) / (self.embed_size ** (1 / 2)), dim = -1)\n",
        "        #(seq_len, attention_size) * (attention_size, seq_len) -> (seq_len, seq_len)\n",
        "        out = attention_score @ value #(seq_len, seq_len) * (seq_len, attention_size) -> (seq_len, attention_size)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "O99B3fm15Cd_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_encoder_decoder_attention():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Define dimensions\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    embed_size = 64\n",
        "    head_num = 8\n",
        "\n",
        "    # Create an instance of SelfAttention\n",
        "    multi_head_attention = MultiHeadAttention(embed_size, head_num)\n",
        "    masekd_multi_head_attention = MaskedMultiHeadAttention(embed_size, head_num)\n",
        "\n",
        "\n",
        "    # Create a random input tensor\n",
        "    x = torch.randn(batch_size, seq_length, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output1, key, value = multi_head_attention(x)\n",
        "    output2, query = masekd_multi_head_attention(x)\n",
        "\n",
        "    encoder_decoder_attention = EncoderDecoderAttention(embed_size, head_num, key, value)\n",
        "\n",
        "    output3 = encoder_decoder_attention(output2)\n",
        "    # Print shapes\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output1 shape: {output1.shape}\")\n",
        "    print(f\"Output2 shape: {output2.shape}\")\n",
        "    print(f\"Output3 shape: {output3.shape}\")\n",
        "    # Check if output shape is correct\n",
        "    assert output1.shape == (batch_size, seq_length, embed_size), \"Output1 shape is incorrect\"\n",
        "    assert output2.shape == (batch_size, seq_length, embed_size), \"Output2 shape is incorrect\"\n",
        "    assert output3.shape == (batch_size, seq_length, embed_size), \"Output3 shape is incorrect\"\n",
        "\n",
        "    # Check if output values are finite\n",
        "    #assert torch.isfinite(output).all(), \"Output contains non-finite values\"\n",
        "\n",
        "    print(\"All checks passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_encoder_decoder_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS-_iGBw5Cgo",
        "outputId": "34e7d8ac-e8f3-4334-f7d7-5b4e30a1d0de"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 10, 64])\n",
            "Output1 shape: torch.Size([32, 10, 64])\n",
            "Output2 shape: torch.Size([32, 10, 64])\n",
            "Output3 shape: torch.Size([32, 10, 64])\n",
            "All checks passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, head_num, hidden_size):\n",
        "        super().__init__()\n",
        "        self.multihead = MultiHeadAttention(embed_size=embed_size, head_num=head_num)\n",
        "        self.feed_forward = FeedForward(embed_size=embed_size, hidden_size=hidden_size)\n",
        "        self.residual_layer1 = ResidualConnection()\n",
        "        self.residual_layer2 = ResidualConnection()\n",
        "\n",
        "    def forward(self, x):\n",
        "        multihead_x, key, value = self.multihead(x)\n",
        "        x = self.residual_layer1(x, multihead_x)\n",
        "        feed_forward_x = self.feed_forward(x)\n",
        "        x = self.residual_layer2(x, feed_forward_x)\n",
        "\n",
        "        return x, key, value\n",
        "\n",
        "embed_size = 256\n",
        "heads = 8\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = EncoderLayer(embed_size, heads, hidden_size)\n",
        "x = torch.rand(10, 20, embed_size)  # (batch_size, sequence_length, embed_size)\n",
        "print(x.shape)\n",
        "output, key, value = encoder(x)\n",
        "print(output.shape)  # Should be (10, 20, embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k71i-jN5CjY",
        "outputId": "ef3d9d13-dafc-4381-c3da-1d1d6af44667"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 256])\n",
            "torch.Size([10, 20, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, head_num, hidden_size, key, value):\n",
        "        super().__init__()\n",
        "        self.K = key\n",
        "        self.V = value\n",
        "        self.masked_multihead = MaskedMultiHeadAttention(embed_size=embed_size, head_num=head_num)\n",
        "        self.edAttention = EncoderDecoderAttention(embed_size=embed_size, head_num=head_num, K=self.K, V=self.V)\n",
        "        self.feed_forward = FeedForward(embed_size=embed_size, hidden_size=hidden_size)\n",
        "        self.residual_layer1 = ResidualConnection()\n",
        "        self.residual_layer2 = ResidualConnection()\n",
        "        self.residual_layer3 = ResidualConnection()\n",
        "\n",
        "    def forward(self, x):\n",
        "        masked_multihead_x, query = self.masked_multihead(x)\n",
        "        x = self.residual_layer1(x, masked_multihead_x)\n",
        "        edAttention_x = self.edAttention(x)\n",
        "        x = self.residual_layer2(x, edAttention_x)\n",
        "        feed_forward_x = self.feed_forward(x)\n",
        "        x = self.residual_layer3(x, feed_forward_x)\n",
        "\n",
        "        return x\n",
        "\n",
        "embed_size = 256\n",
        "heads = 8\n",
        "hidden_size = 512\n",
        "\n",
        "decoder = DecoderLayer(embed_size, heads, hidden_size, key, value)\n",
        "x = torch.rand(10, 20, embed_size)  # (batch_size, sequence_length, embed_size)\n",
        "print(x.shape)\n",
        "output = decoder(x)\n",
        "print(output.shape)  # Should be (10, 20, embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICQmlTKV5Cmk",
        "outputId": "c8ba592f-be20-4c77-c3ed-953a0252f3d7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 256])\n",
            "torch.Size([10, 20, 256])\n"
          ]
        }
      ]
    }
  ]
}